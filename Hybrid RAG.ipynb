{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19db4e39",
   "metadata": {},
   "source": [
    "1. Data Processing:\n",
    "\n",
    "    - Load and preprocess documents using tools like LangChain's document loaders.\n",
    "\n",
    "    - Generate embeddings and store them in your chosen vector database.\n",
    "\n",
    "2. Indexing for Keyword Search:\n",
    "\n",
    "    - Index the same documents in a keyword-based search engine like Elasticsearch.\n",
    "\n",
    "3. Develop Retrieval Logic:\n",
    "\n",
    "    - Implement functions to perform both semantic and keyword searches.\n",
    "\n",
    "    - Merge and rank the results, possibly using reranking models like Cohere's reranker .\n",
    "\n",
    "4. Integrate with LLM:\n",
    "\n",
    "    - Use the combined context to prompt the LLM and generate responses.\n",
    "\n",
    "5. Build the User Interface:\n",
    "\n",
    "    - Develop a user-friendly interface for query input and response display.\n",
    "\n",
    "6. Testing and Evaluation:\n",
    "\n",
    "    - Test the system with various queries to evaluate performance.\n",
    "\n",
    "    - Monitor metrics like response accuracy, latency, and user satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd4f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the linear regression?\n",
      "Linear regression is a type of supervised machine-learning algorithm that learns from labeled datasets and maps the data points with most optimized linear functions which can be used for prediction on new datasets. It assumes that there is a linear relationship between the input and output, meaning the output changes at a constant rate as the input changes. This relationship is represented by a straight line.\n"
     ]
    }
   ],
   "source": [
    "#  Step 1: Import Required Libraries\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "#  Step 2: Set Your Google API Key\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "#  Step 3: Load and Split the PDF Document\n",
    "pdf_loader = PyPDFLoader(\"Regression.pdf\")\n",
    "pdf_pages = pdf_loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(pdf_pages)\n",
    "\n",
    "#  Step 4: Extract Text Content from Documents\n",
    "texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "#  Step 5: Initialize Google Generative AI Embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "#  Step 6: Create and Save FAISS Vector Store\n",
    "vectorstore = FAISS.from_texts(texts, embedding=embeddings)\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "#  Step 7: Load the FAISS Vector Store\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "#  Step 8: Set Up Dense Retriever from FAISS\n",
    "dense_retriever = vectorstore.as_retriever()\n",
    "\n",
    "#  Step 9: Set Up BM25 Retriever (Sparse)\n",
    "bm25_retriever = BM25Retriever.from_documents(split_documents)\n",
    "bm25_retriever.k = 5  # Number of top documents to retrieve\n",
    "\n",
    "#  Step 10: Combine Dense and Sparse Retrievers into a Hybrid Retriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, dense_retriever],\n",
    "    weights=[0.3, 0.7]\n",
    ")\n",
    "\n",
    "#  Step 11: Initialize the Gemini Language Model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "#  Step 12: Create a Prompt Template for the QA System\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {input}\"\n",
    ")\n",
    "\n",
    "#  Step 13: Create a Chain to Combine Retrieved Documents\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "#  Step 14: Create the Retrieval Chain Using the Hybrid Retriever and Combine Docs Chain\n",
    "retrieval_chain = create_retrieval_chain(hybrid_retriever, combine_docs_chain)\n",
    "\n",
    "#  Step 15: Ask a Question and Retrieve the Answer\n",
    "question = \"What is the linear regression?\"\n",
    "response = retrieval_chain.invoke({\"input\": question})\n",
    "\n",
    "#  Step 16: Print the Answer\n",
    "print(response[\"input\"])\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46804b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
